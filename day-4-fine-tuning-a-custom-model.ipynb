{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"day-4-fine-tuning-a-custom-model.ipynb","toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Copyright 2025 Google LLC.","metadata":{"id":"b6e13eef3f5d"}},{"cell_type":"code","source":"# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"cellView":"form","id":"d6597b11df14","jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:39:24.010656Z","iopub.execute_input":"2025-02-28T07:39:24.011577Z","iopub.status.idle":"2025-02-28T07:39:24.035703Z","shell.execute_reply.started":"2025-02-28T07:39:24.011515Z","shell.execute_reply":"2025-02-28T07:39:24.034263Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Day 4 - Fine tuning a custom model\n\nWelcome back to the Kaggle 5-day Generative AI course!\n\nIn this notebook you will use the Gemini API to fine-tune a custom, task-specific model. Fine-tuning can be used for a variety of tasks from classic NLP problems like entity extraction or summarisation, to creative tasks like stylised generation. You will fine-tune a model to classify the category a piece of text (a newsgroup post) into the category it belongs to (the newsgroup name).\n\nThis codelab walks you tuning a model with the API. [AI Studio](https://aistudio.google.com/app/tune) also supports creating new tuned models directly in the web UI, allowing you to quickly create and monitor models using data from Google Sheets, Drive or your own files.\n\n**Note**: We recommend doing this codelab first today. There may be a period of waiting while the model tunes, so if you start with this one, you can try the other codelab while you wait.","metadata":{"id":"4KDIFPAL2EnL"}},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"id":"9wafTyEH1_xF","trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:08:52.646996Z","iopub.execute_input":"2026-02-25T21:08:52.647580Z","iopub.status.idle":"2026-02-25T21:09:05.377649Z","shell.execute_reply.started":"2026-02-25T21:08:52.647508Z","shell.execute_reply":"2026-02-25T21:09:05.375874Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\ngenai.__version__","metadata":{"id":"T0CBG9xL2PvT","trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:09:05.380626Z","iopub.execute_input":"2026-02-25T21:09:05.380990Z","iopub.status.idle":"2026-02-25T21:09:05.390759Z","shell.execute_reply.started":"2026-02-25T21:09:05.380955Z","shell.execute_reply":"2026-02-25T21:09:05.389403Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"### Set up your API key\n\nTo run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n\nIf you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n\nTo make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook.","metadata":{"id":"P4bYX2T72ScK"}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"id":"VuJPY3GK2SLZ","trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:09:08.218753Z","iopub.execute_input":"2026-02-25T21:09:08.219572Z","iopub.status.idle":"2026-02-25T21:09:08.562353Z","shell.execute_reply.started":"2026-02-25T21:09:08.219511Z","shell.execute_reply":"2026-02-25T21:09:08.560587Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"If you received an error response along the lines of `No user secrets exist for kernel id ...`, then you need to add your API key via `Add-ons`, `Secrets` **and** enable it.\n\n![Screenshot of the checkbox to enable GOOGLE_API_KEY secret](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)","metadata":{"id":"25b2127c2052"},"attachments":{}},{"cell_type":"markdown","source":"### Explore available models\n\nYou will be using the [`TunedModel.create`](https://ai.google.dev/api/tuning#method:-tunedmodels.create) API method to start the fine-tuning job and create your custom model. Find a model that supports it through the [`models.list`](https://ai.google.dev/api/models#method:-models.list) endpoint. You can also find more information about tuning models in [the model tuning docs](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python).","metadata":{"id":"CqVA5QFO6n4z"}},{"cell_type":"code","source":"for model in client.models.list():\n    if \"createTunedModel\" in model.supported_actions:\n        print(model.name)","metadata":{"id":"coEacWAB6o0G","trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:09:20.940799Z","iopub.execute_input":"2026-02-25T21:09:20.941546Z","iopub.status.idle":"2026-02-25T21:09:21.017124Z","shell.execute_reply.started":"2026-02-25T21:09:20.941492Z","shell.execute_reply":"2026-02-25T21:09:21.015594Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Download the dataset\n\nIn this activity, you will use the same newsgroups dataset that [you used to train a classifier in Keras](https://www.kaggle.com/code/markishere/day-2-classifying-embeddings-with-keras/). In this example you will use a fine-tuned Gemini model to achieve the same goal.\n\nThe [20 Newsgroups Text Dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) contains 18,000 newsgroups posts on 20 topics divided into training and test sets.","metadata":{"id":"peFm0w_0c1CO"}},{"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\n\nnewsgroups_train = fetch_20newsgroups(subset=\"train\")\nnewsgroups_test = fetch_20newsgroups(subset=\"test\")\n\n# View list of class names for dataset\nnewsgroups_train.target_names","metadata":{"id":"bX_kpgnQ9b-Z","trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:09:54.902271Z","iopub.execute_input":"2026-02-25T21:09:54.903454Z","iopub.status.idle":"2026-02-25T21:10:07.503432Z","shell.execute_reply.started":"2026-02-25T21:09:54.903399Z","shell.execute_reply":"2026-02-25T21:10:07.502231Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"Here's what a single row looks like.","metadata":{"id":"ipafe6ptZFjt"}},{"cell_type":"code","source":"print(newsgroups_train.data[0])","metadata":{"id":"EtEXcdT39hCB","trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:15:35.748427Z","iopub.execute_input":"2026-02-25T21:15:35.749562Z","iopub.status.idle":"2026-02-25T21:15:35.758434Z","shell.execute_reply.started":"2026-02-25T21:15:35.749501Z","shell.execute_reply":"2026-02-25T21:15:35.756487Z"}},"outputs":[{"name":"stdout","text":"From: lerxst@wam.umd.edu (where's my thing)\nSubject: WHAT car is this!?\nNntp-Posting-Host: rac3.wam.umd.edu\nOrganization: University of Maryland, College Park\nLines: 15\n\n I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n\nThanks,\n- IL\n   ---- brought to you by your neighborhood Lerxst ----\n\n\n\n\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Prepare the dataset\n\nYou'll use the same pre-processing code you used for the custom model on day 2. This pre-processing removes personal information, which can be used to \"shortcut\" to known users of a forum, and formats the text to appear a bit more like regular text and less like a newsgroup post (e.g. by removing the mail headers). This normalisation allows the model to generalise to regular text and not over-depend on specific fields. If your input data is always going to be newsgroup posts, it may be helpful to leave this structure in place if they provide genuine signals.","metadata":{"id":"03lDs1O4ZQ0-"}},{"cell_type":"code","source":"import email\nimport re\n\nimport pandas as pd\n\n\ndef preprocess_newsgroup_row(data):\n    # Extract only the subject and body\n    msg = email.message_from_string(data)\n    text = f\"{msg['Subject']}\\n\\n{msg.get_payload()}\"\n    # Strip any remaining email addresses\n    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n    # Truncate the text to fit within the input limits\n    text = text[:40000]\n\n    return text\n\n\ndef preprocess_newsgroup_data(newsgroup_dataset):\n    # Put data points into dataframe\n    df = pd.DataFrame(\n        {\"Text\": newsgroup_dataset.data, \"Label\": newsgroup_dataset.target}\n    )\n    # Clean up the text\n    df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)\n    # Match label to target name index\n    df[\"Class Name\"] = df[\"Label\"].map(lambda l: newsgroup_dataset.target_names[l])\n\n    return df","metadata":{"id":"IoNYTxpoZgB0","trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:16:03.222369Z","iopub.execute_input":"2026-02-25T21:16:03.222933Z","iopub.status.idle":"2026-02-25T21:16:03.231891Z","shell.execute_reply.started":"2026-02-25T21:16:03.222887Z","shell.execute_reply":"2026-02-25T21:16:03.230323Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Apply preprocessing to training and test datasets\ndf_train = preprocess_newsgroup_data(newsgroups_train)\ndf_test = preprocess_newsgroup_data(newsgroups_test)\n\ndf_train.head()","metadata":{"id":"kvOsUSRWaW4g","trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:16:08.212701Z","iopub.execute_input":"2026-02-25T21:16:08.214399Z","iopub.status.idle":"2026-02-25T21:16:12.131591Z","shell.execute_reply.started":"2026-02-25T21:16:08.214316Z","shell.execute_reply":"2026-02-25T21:16:12.130105Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                                                Text  Label  \\\n0  WHAT car is this!?\\n\\n I was wondering if anyo...      7   \n1  SI Clock Poll - Final Call\\n\\nA fair number of...      4   \n2  PB questions...\\n\\nwell folks, my mac plus fin...      4   \n3  Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...      1   \n4  Re: Shuttle Launch Question\\n\\nFrom article <>...     14   \n\n              Class Name  \n0              rec.autos  \n1  comp.sys.mac.hardware  \n2  comp.sys.mac.hardware  \n3          comp.graphics  \n4              sci.space  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Label</th>\n      <th>Class Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WHAT car is this!?\\n\\n I was wondering if anyo...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SI Clock Poll - Final Call\\n\\nA fair number of...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PB questions...\\n\\nwell folks, my mac plus fin...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...</td>\n      <td>1</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Re: Shuttle Launch Question\\n\\nFrom article &lt;&gt;...</td>\n      <td>14</td>\n      <td>sci.space</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"Now sample the data. You will keep 50 rows for each category for training. Note that this is even fewer than the Keras example, as this technique (parameter-efficient fine-tuning, or PEFT) updates a relatively small number of parameters and does not require training a new model or updating the large model.","metadata":{"id":"XSKcj5WtadaR"}},{"cell_type":"code","source":"def sample_data(df, num_samples, classes_to_keep):\n    # Sample rows, selecting num_samples of each Label.\n    df = (\n        df.groupby(\"Label\")[df.columns]\n        .apply(lambda x: x.sample(num_samples))\n        .reset_index(drop=True)\n    )\n\n    df = df[df[\"Class Name\"].str.contains(classes_to_keep)]\n    df[\"Class Name\"] = df[\"Class Name\"].astype(\"category\")\n\n    return df\n\n\nTRAIN_NUM_SAMPLES = 50\nTEST_NUM_SAMPLES = 10\n# Keep rec.* and sci.*\nCLASSES_TO_KEEP = \"^rec|^sci\"\n\ndf_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP)\ndf_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP)","metadata":{"id":"0t9Xu6X5akkt","trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:16:14.180209Z","iopub.execute_input":"2026-02-25T21:16:14.180943Z","iopub.status.idle":"2026-02-25T21:16:14.214145Z","shell.execute_reply.started":"2026-02-25T21:16:14.180898Z","shell.execute_reply":"2026-02-25T21:16:14.212775Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## Evaluate baseline performance\n\nBefore you start tuning a model, it's good practice to perform an evaluation on the available models to ensure you can measure how much the tuning helps.\n\nFirst identify a single sample row to use for visual inspection.","metadata":{}},{"cell_type":"code","source":"sample_idx = 0\nsample_row = preprocess_newsgroup_row(newsgroups_test.data[sample_idx])\nsample_label = newsgroups_test.target_names[newsgroups_test.target[sample_idx]]\n\nprint(sample_row)\nprint('---')\nprint('Label:', sample_label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:16:16.902436Z","iopub.execute_input":"2026-02-25T21:16:16.902912Z","iopub.status.idle":"2026-02-25T21:16:16.911267Z","shell.execute_reply.started":"2026-02-25T21:16:16.902876Z","shell.execute_reply":"2026-02-25T21:16:16.909543Z"}},"outputs":[{"name":"stdout","text":"Need info on 88-89 Bonneville\n\n\n I am a little confused on all of the models of the 88-89 bonnevilles.\nI have heard of the LE SE LSE SSE SSEI. Could someone tell me the\ndifferences are far as features or performance. I am also curious to\nknow what the book value is for prefereably the 89 model. And how much\nless than book value can you usually get them for. In other words how\nmuch are they in demand this time of year. I have heard that the mid-spring\nearly summer is the best time to buy.\n\n\t\t\tNeil Gandler\n\n---\nLabel: rec.autos\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"Passing the text directly in as a prompt does not yield the desired results. The model will attempt to respond to the message.","metadata":{}},{"cell_type":"code","source":"response = client.models.generate_content(\n    model=\"gemini-2.5-flash\", contents=sample_row)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:16:19.360043Z","iopub.execute_input":"2026-02-25T21:16:19.360681Z","iopub.status.idle":"2026-02-25T21:16:42.576579Z","shell.execute_reply.started":"2026-02-25T21:16:19.360637Z","shell.execute_reply":"2026-02-25T21:16:42.574768Z"}},"outputs":[{"name":"stdout","text":"Hi Neil,\n\nIt's understandable you're a little confused! Pontiac did use a somewhat convoluted naming scheme for the Bonneville during that era, and model variations can be subtle. Let's break down the 1988-1989 Bonneville models, their features, and then tackle the pricing questions.\n\nThe 1988-1989 Bonneville was built on GM's H-body platform (shared with the Buick LeSabre and Oldsmobile Delta 88). All models for these years came standard with the **3.8L (3800 Series I) V6 engine**, producing around 165 horsepower and 210 lb-ft of torque, mated to a 4-speed automatic transmission (the 4T60). Performance differences between the models are primarily in handling and features, not engine output.\n\n---\n\n### 1988-1989 Bonneville Models & Differences:\n\nHere's a general hierarchy and breakdown of features:\n\n#### 1. Bonneville LE (Luxury Edition)\n*   **Position:** This was the base model, focused on comfort and value.\n*   **Features:**\n    *   Standard cloth interior.\n    *   Less equipped dashboard (analog gauges, fewer features).\n    *   More comfort-oriented suspension tuning (softer ride).\n    *   Often had steel wheels with hubcaps.\n    *   Fewer power accessories standard (e.g., manual seats, crank windows were possible but less common).\n*   **Performance:** Same engine, but the softer suspension means less agile handling than sportier trims.\n\n#### 2. Bonneville SE (Sport Edition)\n*   **Position:** A step up from the LE, adding a sportier appearance and more features.\n*   **Features:**\n    *   Nicer cloth or optional leather interior.\n    *   Alloy wheels were usually standard or a common option.\n    *   Firmer suspension tuning than the LE, but not as aggressive as the SSE.\n    *   More standard power accessories (power windows, locks, often power driver's seat).\n    *   Often featured subtle body accents like blacked-out trim.\n*   **Performance:** Same engine. Improved handling over the LE due to firmer suspension and potentially wider tires, but still a comfortable cruiser.\n\n#### 3. Bonneville LSE (Luxury Sport Edition)\n*   **Position:** This was a relatively rare trim, often only offered for a year or two (1989 was a common year for it). It aimed to blend the luxury features of the LE with some of the sporty aesthetics of the SE, but without going as aggressive as the SSE.\n*   **Features:**\n    *   Often featured unique interior trim, special paint colors, or a specific leather package.\n    *   Usually came with alloy wheels.\n    *   Suspension tuning was typically somewhere between the LE and SE, leaning towards comfort but with decent road manners.\n    *   More luxury options standard than an SE.\n*   **Performance:** Same engine, generally a refined ride, a bit softer than an SSE.\n\n#### 4. Bonneville SSE (Sport Sedan, Enhanced)\n*   **Position:** The top-of-the-line performance and luxury model for these years.\n*   **Features:**\n    *   Aggressive body cladding/ground effects, distinctive front fascia with fog lights.\n    *   Full instrumentation, often with a digital dashboard and/or a Heads-Up Display (HUD - the Bonneville SSE was one of the first cars to offer this, starting in 1988).\n    *   Standard leather bucket seats, often with power adjustments for both driver and passenger.\n    *   Performance-tuned suspension (FE3 package), offering significantly better handling and a firmer ride.\n    *   Larger, unique alloy wheels (e.g., 16-inch).\n    *   Standard Anti-lock Brakes (ABS).\n    *   Premium sound system.\n    *   Integrated rear spoiler.\n*   **Performance:** Same 3.8L V6 engine, but the suspension, larger wheels/tires, and standard ABS significantly improved its handling, braking, and overall driving dynamics compared to the other trims. It was marketed as a \"4-door Corvette\" by Pontiac at the time.\n\n#### 5. Bonneville SSEi\n*   **CRITICAL DISTINCTION:** The **SSEi was NOT available in 1988 or 1989.** The SSEi model, featuring the supercharged 3.8L V6 engine, was introduced later, starting with the completely redesigned 1992 Bonneville. If you've heard of an '88 or '89 SSEi, it's a misunderstanding; the \"i\" indicating \"intercooled\" (and therefore supercharged) didn't apply to these years. All 88-89 Bonnevilles were naturally aspirated.\n\n---\n\n### Book Value for an 1989 Bonneville:\n\nThis is where things get a bit blunt. For a car from 1989, \"book value\" (from sources like Kelley Blue Book or NADAguides) is often very low, primarily because of its age and the sheer number of these cars originally sold.\n\n*   **Typical Range:** For a 1989 Bonneville, even in good condition, you're looking at a **private party value often between $500 and $1,500.**\n    *   An **LE or SE** in average to good shape might fetch **$500-$1000**.\n    *   An **SSE** in truly excellent, well-maintained condition (which is rare for a car of this age) could *potentially* go for **$1,200-$2,000**, but this is an outlier. Most will be at the lower end.\n    *   Condition, mileage, and maintenance records are *everything* for a car this old. A non-running or rough example could be $300 or less.\n\n*   **Book Value vs. Reality:** For cars this old, \"book value\" is more of a formality. The real value is what a willing buyer and a willing seller agree upon. Most of these cars are sold for cash, and their primary value is as affordable, reliable transportation, not as collector's items.\n\n### How Much Less Than Book Value Can You Get Them For?\n\n*   Given that \"book value\" is already so low, there isn't much room to go \"less than book\" in the traditional sense. Many sellers will list their car for a bit *more* than what KBB suggests because they feel it's worth more (e.g., they've put new tires on it, done recent repairs).\n*   **Negotiation is key.** If a car is listed for $1,500 and KBB says $800, you have strong grounds to negotiate. However, if it's listed for $800, you might only be able to knock off $100-$200 unless there are significant issues.\n*   **Focus on the car's condition and needs.** Any issues (rust, failing components, worn interior) are excellent negotiation points.\n\n### Demand This Time of Year & Best Time to Buy:\n\n*   **Demand for an 88-89 Bonneville:** Demand is generally very low. These cars are primarily sought by:\n    *   Someone looking for cheap, reliable transportation.\n    *   Someone specifically nostalgic for the model.\n    *   Someone who appreciates the robust 3800 V6 engine.\n    *   It's not a \"hot commodity\" vehicle.\n*   **Mid-Spring/Early Summer Buying:**\n    *   **General Rule:** It's often said that spring/summer is a better time to *sell* cars because more people are out looking, the weather is good for test drives, and tax returns might fuel purchases. Conversely, fall/winter can be better for *buying* as demand might dip slightly.\n    *   **For an 88-89 Bonneville:** For a car of this age and price point, seasonal demand fluctuations will have a **minimal impact** on the price. You're not buying a convertible or an SUV where seasonal demand significantly moves the needle.\n    *   **Best Time to Buy:** The \"best time to buy\" an 88-89 Bonneville is simply **whenever you find one in excellent mechanical condition that fits your budget.** Don't wait for a specific season if a good deal on a well-maintained car pops up. The rarity of truly *good* examples is more of a factor than the calendar date.\n\n### Important Considerations When Buying:\n\n*   **Rust:** Check wheel wells, rocker panels, under doors, and the subframe thoroughly.\n*   **Maintenance:** Look for evidence of regular oil changes, transmission fluid changes, and tune-ups.\n*   **Common Issues:** Listen for transmission shifts (should be smooth, not harsh), check for oil leaks, inspect all hoses and belts, and test all electrical components (especially on the SSE with its more complex electronics).\n*   **Mileage:** Expect high mileage, but a well-maintained high-mileage 3800 V6 is often better than a poorly maintained low-mileage one.\n*   **Pre-Purchase Inspection:** If you're serious about a specific car, a pre-purchase inspection by a trusted mechanic is highly recommended. It's $100 well spent to avoid major headaches.\n\nGood luck with your search, Neil! The 3800 V6 is a great engine, and a well-kept Bonneville from this era can still provide reliable and comfortable transportation.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"You can use the prompt engineering techniques you have learned this week to induce the model to perform the desired task. Try some of your own ideas and see what is effective, or check out the following cells for different approaches. Note that they have different levels of effectiveness!","metadata":{}},{"cell_type":"code","source":"# Ask the model directly in a zero-shot prompt.\n\nprompt = \"From what newsgroup does the following message originate?\"\nbaseline_response = client.models.generate_content(\n    model=\"gemini-2.5-flash\",\n    contents=[prompt, sample_row])\nprint(baseline_response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:16:42.579018Z","iopub.execute_input":"2026-02-25T21:16:42.579601Z","iopub.status.idle":"2026-02-25T21:16:47.116157Z","shell.execute_reply.started":"2026-02-25T21:16:42.579542Z","shell.execute_reply":"2026-02-25T21:16:47.113903Z"}},"outputs":[{"name":"stdout","text":"The message most likely originates from the newsgroup **rec.autos.misc** or a similar automotive-related newsgroup such as **rec.autos**.\n\n`rec.autos.misc` is a general discussion group for various automotive topics that don't fit into more specific `rec.autos` subgroups (like `rec.autos.tech` for technical questions or `rec.autos.marketplace` for buying/selling specific items). This message, asking about models, features, performance, value, and buying advice, fits perfectly within the scope of `rec.autos.misc`.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"This technique still produces quite a verbose response. You could try and parse out the relevant text, or refine the prompt even further.","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\n\n# You can use a system instruction to do more direct prompting, and get a\n# more succinct answer.\n\nsystem_instruct = \"\"\"\nYou are a classification service. You will be passed input that represents\na newsgroup post and you must respond with the newsgroup from which the post\noriginates.\n\"\"\"\n\n# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n# If you want to evaluate your own technique, replace this body of this function\n# with your model, prompt and other code and return the predicted answer.\n@retry.Retry(predicate=is_retriable)\ndef predict_label(post: str) -> str:\n    response = client.models.generate_content(\n        model=\"gemini-2.5-flash\",\n        config=types.GenerateContentConfig(\n            system_instruction=system_instruct),\n        contents=post)\n\n    rc = response.candidates[0]\n\n    # Any errors, filters, recitation, etc we can mark as a general error\n    if rc.finish_reason.name != \"STOP\":\n        return \"(error)\"\n    else:\n        # Clean up the response.\n        return response.text.strip()\n\n\nprediction = predict_label(sample_row)\n\nprint(prediction)\nprint()\nprint(\"Correct!\" if prediction == sample_label else \"Incorrect.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:19:14.057768Z","iopub.execute_input":"2026-02-25T21:19:14.058328Z","iopub.status.idle":"2026-02-25T21:19:15.031450Z","shell.execute_reply.started":"2026-02-25T21:19:14.058286Z","shell.execute_reply":"2026-02-25T21:19:15.030039Z"}},"outputs":[{"name":"stdout","text":"rec.autos\n\nCorrect!\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"Now run a short evaluation using the function defined above. The test set is further sampled to ensure the experiment runs smoothly on the API's free tier. In practice you would evaluate over the whole set.","metadata":{}},{"cell_type":"code","source":"import tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas.\ntqdmr.pandas()\n\n# But suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n\n# Further sample the test data to be mindful of the free-tier quota.\ndf_baseline_eval = sample_data(df_test, 2, '.*')\n\n# Make predictions using the sampled data.\ndf_baseline_eval['Prediction'] = df_baseline_eval['Text'].progress_apply(predict_label)\n\n# And calculate the accuracy.\naccuracy = (df_baseline_eval[\"Class Name\"] == df_baseline_eval[\"Prediction\"]).sum() / len(df_baseline_eval)\nprint(f\"Accuracy: {accuracy:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:19:18.724870Z","iopub.execute_input":"2026-02-25T21:19:18.726564Z","iopub.status.idle":"2026-02-25T21:19:32.369792Z","shell.execute_reply.started":"2026-02-25T21:19:18.726496Z","shell.execute_reply":"2026-02-25T21:19:32.368436Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d0084fa3bfa45bab54de424aec3192e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Accuracy: 37.50%\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"Now take a look at the dataframe to compare the predictions with the labels.","metadata":{}},{"cell_type":"code","source":"df_baseline_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:19:32.377239Z","iopub.execute_input":"2026-02-25T21:19:32.377820Z","iopub.status.idle":"2026-02-25T21:19:32.399215Z","shell.execute_reply.started":"2026-02-25T21:19:32.377759Z","shell.execute_reply":"2026-02-25T21:19:32.397128Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"                                                 Text  Label  \\\n0   VASCAR\\n\\nI know this is the wrong place to po...      7   \n1   Re: Mercury Villager Minivan -- good buy?\\n\\nI...      7   \n2   Re: dogs\\n\\nIn article <>,  (Charles Parr) wri...      8   \n3   Re: DoD Pins...NOT!\\n\\nIn article <>  writes:\\...      8   \n4   Re: Neon Deon Sanders  (Braves & Giants)\\n\\nIn...      9   \n5   Mea Culpa -- Bosio no-no\\n\\nLike Clinton and R...      9   \n6   Re: Grant Fuhr leads Sabres\\n\\nIn <> \\n(Deepak...     10   \n7   Buffalo Sabres\\n\\n  Ok, Buffalo fans.  I am a ...     10   \n8   Re: PGP Where to get it?\\n\\n>At the moment PGP...     11   \n9   Re: Wiretapping reality today\\n\\n-----BEGIN PG...     11   \n10  WD-40 as moisture repellant (was Lead Acid bat...     12   \n11  HELP!  Looking for Oscilloscope source\\n\\nHi a...     12   \n12  Re: Candida(yeast) Bloom, Fact or Fiction\\n\\nI...     13   \n13  Density of the skull bone\\n\\nCould someone tel...     13   \n14  Two-Line Orbital Element Set:  Space Shuttle\\n...     14   \n15  Re: Drag Free Satellites\\n\\nRegarding drag fre...     14   \n\n            Class Name            Prediction  \n0            rec.autos             soc.motss  \n1            rec.autos             rec.autos  \n2      rec.motorcycles       `rec.pets.dogs`  \n3      rec.motorcycles       rec.motorcycles  \n4   rec.sport.baseball    rec.sport.baseball  \n5   rec.sport.baseball  `rec.sport.baseball`  \n6     rec.sport.hockey      rec.sport.hockey  \n7     rec.sport.hockey      rec.sport.hockey  \n8            sci.crypt     comp.security.pgp  \n9            sci.crypt          soc.politics  \n10     sci.electronics           `rec.autos`  \n11     sci.electronics     `sci.electronics`  \n12             sci.med             `sci.med`  \n13             sci.med               sci.med  \n14           sci.space           `sci.space`  \n15           sci.space               SCIENCE  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Label</th>\n      <th>Class Name</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>VASCAR\\n\\nI know this is the wrong place to po...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n      <td>soc.motss</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Re: Mercury Villager Minivan -- good buy?\\n\\nI...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n      <td>rec.autos</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Re: dogs\\n\\nIn article &lt;&gt;,  (Charles Parr) wri...</td>\n      <td>8</td>\n      <td>rec.motorcycles</td>\n      <td>`rec.pets.dogs`</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Re: DoD Pins...NOT!\\n\\nIn article &lt;&gt;  writes:\\...</td>\n      <td>8</td>\n      <td>rec.motorcycles</td>\n      <td>rec.motorcycles</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Re: Neon Deon Sanders  (Braves &amp; Giants)\\n\\nIn...</td>\n      <td>9</td>\n      <td>rec.sport.baseball</td>\n      <td>rec.sport.baseball</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Mea Culpa -- Bosio no-no\\n\\nLike Clinton and R...</td>\n      <td>9</td>\n      <td>rec.sport.baseball</td>\n      <td>`rec.sport.baseball`</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Re: Grant Fuhr leads Sabres\\n\\nIn &lt;&gt; \\n(Deepak...</td>\n      <td>10</td>\n      <td>rec.sport.hockey</td>\n      <td>rec.sport.hockey</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Buffalo Sabres\\n\\n  Ok, Buffalo fans.  I am a ...</td>\n      <td>10</td>\n      <td>rec.sport.hockey</td>\n      <td>rec.sport.hockey</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Re: PGP Where to get it?\\n\\n&gt;At the moment PGP...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>comp.security.pgp</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Re: Wiretapping reality today\\n\\n-----BEGIN PG...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>soc.politics</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>WD-40 as moisture repellant (was Lead Acid bat...</td>\n      <td>12</td>\n      <td>sci.electronics</td>\n      <td>`rec.autos`</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>HELP!  Looking for Oscilloscope source\\n\\nHi a...</td>\n      <td>12</td>\n      <td>sci.electronics</td>\n      <td>`sci.electronics`</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Re: Candida(yeast) Bloom, Fact or Fiction\\n\\nI...</td>\n      <td>13</td>\n      <td>sci.med</td>\n      <td>`sci.med`</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Density of the skull bone\\n\\nCould someone tel...</td>\n      <td>13</td>\n      <td>sci.med</td>\n      <td>sci.med</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Two-Line Orbital Element Set:  Space Shuttle\\n...</td>\n      <td>14</td>\n      <td>sci.space</td>\n      <td>`sci.space`</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Re: Drag Free Satellites\\n\\nRegarding drag fre...</td>\n      <td>14</td>\n      <td>sci.space</td>\n      <td>SCIENCE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"## Tune a custom model\n\nIn this example you'll use tuning to create a model that requires no prompting or system instructions and outputs succinct text from the classes you provide in the training data.\n\nThe data contains both input text (the processed posts) and output text (the category, or newsgroup), that you can use to start tuning a model.\n\nWhen calling `tune()`, you can specify model tuning hyperparameters too:\n - `epoch_count`: defines how many times to loop through the data,\n - `batch_size`: defines how many rows to process in a single step, and\n - `learning_rate`: defines the scaling factor for updating model weights at each step.\n\nYou can also choose to omit them and use the defaults. [Learn more](https://developers.google.com/machine-learning/crash-course/linear-regression/hyperparameters) about these parameters and how they work. For this example these parameters were selected by running some tuning jobs and selecting parameters that converged efficiently.\n\nThis example will start a new tuning job, but only if one does not already exist. This allows you to leave this codelab and come back later - re-running this step will find your last model.","metadata":{"id":"Ok7ugrLzcghX"}},{"cell_type":"code","source":"### from collections.abc import Iterable\n## import random\n\n\n# Convert the data frame into a dataset suitable for tuning.\n## input_data = {'examples': \n##    df_train[['Text', 'Class Name']]\n##.rename(columns={'Text': 'textInput', 'Class Name': 'output'})\n##      .to_dict(orient='records')\n## }\n\n# If you are re-running this lab, add your model_id here.\n##model_id = None\n\n# # Or try and find a recent tuning job.\n# if not model_id:\n#   queued_model = None\n#   # Newest models first.\n#   for m in reversed(client.tunings.list()):\n#     # Only look at newsgroup classification models.\n#     if m.name.startswith('tunedModels/newsgroup-classification-model'):\n#       # If there is a completed model, use the first (newest) one.\n#       if m.state.name == 'JOB_STATE_SUCCEEDED':\n#         model_id = m.name\n#         print('Found existing tuned model to reuse.')\n#         break\n\n#       elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n#         # If there's a model still queued, remember the most recent one.\n#         queued_model = m.name\n#   else:\n#     if queued_model:\n#       model_id = queued_model\n#       print('Found queued model, still waiting.')\n\n\n# # Upload the training data and queue the tuning job.\n# if not model_id:\n#     tuning_op = client.tunings.tune(\n#         base_model=\"models/gemini-1.5-flash-001-tuning\",\n#         training_dataset=input_data,\n#         config=types.CreateTuningJobConfig(\n#             tuned_model_display_name=\"Newsgroup classification model\",\n#             batch_size=16,\n#             epoch_count=2,\n#         ),\n#     )\n\n#     print(tuning_op.state)\n#     model_id = tuning_op.name\n\n# print(model_id)","metadata":{"id":"pWOZlspfY8dV","trusted":true,"execution":{"iopub.status.busy":"2026-02-25T21:22:31.237790Z","iopub.execute_input":"2026-02-25T21:22:31.239132Z","iopub.status.idle":"2026-02-25T21:22:31.246464Z","shell.execute_reply.started":"2026-02-25T21:22:31.239071Z","shell.execute_reply":"2026-02-25T21:22:31.244793Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"encontrados = []\n\n# Listamos todos os modelos e filtramos pela permissão de tuning\nfor m in client.models.list():\n    # Verificamos se 'createTunedModel' está nas ações suportadas\n    if any('createTunedModel' in action or 'create_tuned_model' in action for action in m.supported_actions):\n        print(f\"✅ MODELO ENCONTRADO: {m.name}\")\n        encontrados.append(m.name)\n\nif not encontrados:\n    print(\"❌ Nenhum modelo de tuning disponível para esta API Key.\")\n    print(\"Verifique se o faturamento (billing) está ativo no console do Google Cloud vinculado.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T20:43:46.599398Z","iopub.execute_input":"2026-02-24T20:43:46.599840Z","iopub.status.idle":"2026-02-24T20:43:46.814541Z","shell.execute_reply.started":"2026-02-24T20:43:46.599802Z","shell.execute_reply":"2026-02-24T20:43:46.813457Z"}},"outputs":[{"name":"stdout","text":"❌ Nenhum modelo de tuning disponível para esta API Key.\nVerifique se o faturamento (billing) está ativo no console do Google Cloud vinculado.\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"compatíveis = []\n\nfor m in client.models.list():\n    # No novo SDK, a propriedade pode variar, vamos checar 'create_tuned_model'\n    if 'createTunedModel' in m.supported_actions or 'create_tuned_model' in str(m.supported_actions).lower():\n        print(f\"✅ DISPONÍVEL: {m.name}\")\n        compatíveis.append(m.name)\n\nif not compatíveis:\n    print(\"❌ Nenhum modelo de tuning encontrado na sua região/projeto.\")\n    print(\"Dica: Tente usar 'models/gemini-1.5-flash-001' diretamente.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T20:43:48.978788Z","iopub.execute_input":"2026-02-24T20:43:48.979174Z","iopub.status.idle":"2026-02-24T20:43:49.052738Z","shell.execute_reply.started":"2026-02-24T20:43:48.979140Z","shell.execute_reply":"2026-02-24T20:43:49.051607Z"}},"outputs":[{"name":"stdout","text":"❌ Nenhum modelo de tuning encontrado na sua região/projeto.\nDica: Tente usar 'models/gemini-1.5-flash-001' diretamente.\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"### NÃO HÁ MAIS MODELOS DE TUNING DISPONÍVEIS PELO AI STUDIO\n\nCom a descontinuação do Gemini 1.5 Flash-001 em maio de 2025, não teremos mais um modelo disponível que ofereça suporte ao ajuste fino na API Gemini ou no AI Studio, mas ele é compatível com a Vertex AI.\n\nPlanejamos trazer de volta o suporte ao ajuste refinado no futuro. Gostaríamos de receber sua opinião no fórum de desenvolvedores se o ajuste fino for importante para seu caso de uso.","metadata":{}},{"cell_type":"code","source":"for model in client.models.list():\n    print(f\"Nome: {model.name} - Suporta Tuning: {model.supported_actions}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T14:17:39.809359Z","iopub.execute_input":"2026-02-24T14:17:39.809722Z","iopub.status.idle":"2026-02-24T14:17:40.011998Z","shell.execute_reply.started":"2026-02-24T14:17:39.809690Z","shell.execute_reply":"2026-02-24T14:17:40.010640Z"}},"outputs":[{"name":"stdout","text":"Nome: models/gemini-2.5-flash - Suporta Tuning: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\nNome: models/gemini-2.5-pro - Suporta Tuning: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\nNome: models/gemini-2.0-flash - Suporta Tuning: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\nNome: models/gemini-2.0-flash-001 - Suporta Tuning: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\nNome: models/gemini-2.0-flash-exp-image-generation - Suporta Tuning: ['generateContent', 'countTokens', 'bidiGenerateContent']\nNome: models/gemini-2.0-flash-lite-001 - Suporta Tuning: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\nNome: models/gemini-2.0-flash-lite - Suporta Tuning: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\nNome: models/gemini-2.5-flash-preview-tts - Suporta Tuning: ['countTokens', 'generateContent']\nNome: models/gemini-2.5-pro-preview-tts - Suporta Tuning: ['countTokens', 'generateContent', 'batchGenerateContent']\nNome: models/gemma-3-1b-it - Suporta Tuning: ['generateContent', 'countTokens']\nNome: models/gemma-3-4b-it - Suporta Tuning: ['generateContent', 'countTokens']\nNome: models/gemma-3-12b-it - Suporta Tuning: ['generateContent', 'countTokens']\nNome: models/gemma-3-27b-it - Suporta Tuning: ['generateContent', 'countTokens']\nNome: models/gemma-3n-e4b-it - Suporta Tuning: ['generateContent', 'countTokens']\nNome: models/gemma-3n-e2b-it - Suporta Tuning: ['generateContent', 'countTokens']\nNome: models/gemini-flash-latest - Suporta Tuning: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\nNome: models/gemini-flash-lite-latest - Suporta Tuning: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\nNome: models/gemini-pro-latest - Suporta Tuning: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\nNome: models/gemini-2.5-flash-lite - Suporta Tuning: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\nNome: models/gemini-2.5-flash-image - Suporta Tuning: ['generateContent', 'countTokens', 'batchGenerateContent']\nNome: models/gemini-2.5-flash-lite-preview-09-2025 - Suporta Tuning: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\nNome: models/gemini-3-pro-preview - Suporta Tuning: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\nNome: models/gemini-3-flash-preview - Suporta Tuning: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\nNome: models/gemini-3.1-pro-preview - Suporta Tuning: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\nNome: models/gemini-3.1-pro-preview-customtools - Suporta Tuning: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\nNome: models/gemini-3-pro-image-preview - Suporta Tuning: ['generateContent', 'countTokens', 'batchGenerateContent']\nNome: models/nano-banana-pro-preview - Suporta Tuning: ['generateContent', 'countTokens', 'batchGenerateContent']\nNome: models/gemini-robotics-er-1.5-preview - Suporta Tuning: ['generateContent', 'countTokens']\nNome: models/gemini-2.5-computer-use-preview-10-2025 - Suporta Tuning: ['generateContent', 'countTokens']\nNome: models/deep-research-pro-preview-12-2025 - Suporta Tuning: ['generateContent', 'countTokens']\nNome: models/gemini-embedding-001 - Suporta Tuning: ['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent']\nNome: models/aqa - Suporta Tuning: ['generateAnswer']\nNome: models/imagen-4.0-generate-001 - Suporta Tuning: ['predict']\nNome: models/imagen-4.0-ultra-generate-001 - Suporta Tuning: ['predict']\nNome: models/imagen-4.0-fast-generate-001 - Suporta Tuning: ['predict']\nNome: models/veo-2.0-generate-001 - Suporta Tuning: ['predictLongRunning']\nNome: models/veo-3.0-generate-001 - Suporta Tuning: ['predictLongRunning']\nNome: models/veo-3.0-fast-generate-001 - Suporta Tuning: ['predictLongRunning']\nNome: models/veo-3.1-generate-preview - Suporta Tuning: ['predictLongRunning']\nNome: models/veo-3.1-fast-generate-preview - Suporta Tuning: ['predictLongRunning']\nNome: models/gemini-2.5-flash-native-audio-latest - Suporta Tuning: ['countTokens', 'bidiGenerateContent']\nNome: models/gemini-2.5-flash-native-audio-preview-09-2025 - Suporta Tuning: ['countTokens', 'bidiGenerateContent']\nNome: models/gemini-2.5-flash-native-audio-preview-12-2025 - Suporta Tuning: ['countTokens', 'bidiGenerateContent']\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"This has created a tuning job that will run in the background. To inspect the progress of the tuning job, run this cell to plot the current status and loss curve. Once the status reaches `ACTIVE`, tuning is complete and the model is ready to use.\n\nTuning jobs are queued, so it may look like no training steps have been taken initially but it will progress. Tuning can take anywhere from a few minutes to multiple hours, depending on factors like your dataset size and how busy the tuning infrastrature is. Why not treat yourself to a nice cup of tea while you wait, or come and say \"Hi!\" in the group [Discord](https://discord.com/invite/kaggle).\n\nIt is safe to stop this cell at any point. It will not stop the tuning job.\n\n**IMPORTANT**: Due to the high volume of users doing this course, tuning jobs may be queued for many hours. Take a note of your tuned model ID above (`tunedModels/...`) so you can come back to it tomorrow. In the meantime, check out the [Search grounding](https://www.kaggle.com/code/markishere/day-4-google-search-grounding/) codelab. If you want to try tuning a local LLM, check out [the fine-tuning guides for tuning a Gemma model](https://ai.google.dev/gemma/docs/tune).","metadata":{"id":"NQ3YZ2MBubCY"}},{"cell_type":"code","source":"import datetime\nimport time\n\n\nMAX_WAIT = datetime.timedelta(minutes=10)\n\nwhile not (tuned_model := client.tunings.get(name=model_id)).has_ended:\n\n    print(tuned_model.state)\n    time.sleep(60)\n\n    # Don't wait too long. Use a public model if this is going to take a while.\n    if datetime.datetime.now(datetime.timezone.utc) - tuned_model.create_time > MAX_WAIT:\n        print(\"Taking a shortcut, using a previously prepared model.\")\n        model_id = \"tunedModels/newsgroup-classification-model-ltenbi1b\"\n        tuned_model = client.tunings.get(name=model_id)\n        break\n\n\nprint(f\"Done! The model state is: {tuned_model.state.name}\")\n\nif not tuned_model.has_succeeded and tuned_model.error:\n    print(\"Error:\", tuned_model.error)","metadata":{"id":"c4ef5f13692d","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:11:13.878313Z","iopub.execute_input":"2025-04-02T08:11:13.878725Z","iopub.status.idle":"2025-04-02T08:11:15.417549Z","shell.execute_reply.started":"2025-04-02T08:11:13.878693Z","shell.execute_reply":"2025-04-02T08:11:15.416441Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Use the new model\n\nNow that you have a tuned model, try it out with custom data. You use the same API as a normal Gemini API interaction, but you specify your new model as the model name, which will start with `tunedModels/`.","metadata":{"id":"9-qiIdK4u80z"}},{"cell_type":"code","source":"new_text = \"\"\"\nFirst-timer looking to get out of here.\n\nHi, I'm writing about my interest in travelling to the outer limits!\n\nWhat kind of craft can I buy? What is easiest to access from this 3rd rock?\n\nLet me know how to do that please.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model=model_id, contents=new_text)\n\nprint(response.text)","metadata":{"id":"hyO2-MXLvM6a","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:11:24.915979Z","iopub.execute_input":"2025-04-02T08:11:24.91659Z","iopub.status.idle":"2025-04-02T08:11:27.566345Z","shell.execute_reply.started":"2025-04-02T08:11:24.91654Z","shell.execute_reply":"2025-04-02T08:11:27.564861Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation\n\nYou can see that the model outputs labels that correspond to those in the training data, and without any system instructions or prompting, which is already a great improvement. Now see how well it performs on the test set.\n\nNote that there is no parallelism in this example; classifying the test sub-set will take a few minutes.","metadata":{"id":"xajLek9DySH_"}},{"cell_type":"code","source":"@retry.Retry(predicate=is_retriable)\ndef classify_text(text: str) -> str:\n    \"\"\"Classify the provided text into a known newsgroup.\"\"\"\n    response = client.models.generate_content(\n        model=model_id, contents=text)\n    rc = response.candidates[0]\n\n    # Any errors, filters, recitation, etc we can mark as a general error\n    if rc.finish_reason.name != \"STOP\":\n        return \"(error)\"\n    else:\n        return rc.content.parts[0].text\n\n\n# The sampling here is just to minimise your quota usage. If you can, you should\n# evaluate the whole test set with `df_model_eval = df_test.copy()`.\ndf_model_eval = sample_data(df_test, 4, '.*')\n\ndf_model_eval[\"Prediction\"] = df_model_eval[\"Text\"].progress_apply(classify_text)\n\naccuracy = (df_model_eval[\"Class Name\"] == df_model_eval[\"Prediction\"]).sum() / len(df_model_eval)\nprint(f\"Accuracy: {accuracy:.2%}\")","metadata":{"id":"6T2Y3ZApvbMw","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:11:29.712728Z","iopub.execute_input":"2025-04-02T08:11:29.713129Z","iopub.status.idle":"2025-04-02T08:14:02.348228Z","shell.execute_reply.started":"2025-04-02T08:11:29.713082Z","shell.execute_reply":"2025-04-02T08:14:02.347128Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compare token usage\n\nAI Studio and the Gemini API provide model tuning at no cost, however normal limits and charges apply for *use* of a tuned model.\n\nThe size of the input prompt and other generation config like system instructions, as well as the number of generated output tokens, all contribute to the overall cost of a request.","metadata":{}},{"cell_type":"code","source":"# Calculate the *input* cost of the baseline model with system instructions.\nsysint_tokens = client.models.count_tokens(\n    model='gemini-1.5-flash-001', contents=[system_instruct, sample_row]\n).total_tokens\nprint(f'System instructed baseline model: {sysint_tokens} (input)')\n\n# Calculate the input cost of the tuned model.\ntuned_tokens = client.models.count_tokens(model=tuned_model.base_model, contents=sample_row).total_tokens\nprint(f'Tuned model: {tuned_tokens} (input)')\n\nsavings = (sysint_tokens - tuned_tokens) / tuned_tokens\nprint(f'Token savings: {savings:.2%}')  # Note that this is only n=1.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:14:02.35288Z","iopub.execute_input":"2025-04-02T08:14:02.353306Z","iopub.status.idle":"2025-04-02T08:14:02.612742Z","shell.execute_reply.started":"2025-04-02T08:14:02.353257Z","shell.execute_reply":"2025-04-02T08:14:02.611517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The earlier verbose model also produced more output tokens than needed for this task.","metadata":{}},{"cell_type":"code","source":"baseline_token_output = baseline_response.usage_metadata.candidates_token_count\nprint('Baseline (verbose) output tokens:', baseline_token_output)\n\ntuned_model_output = client.models.generate_content(\n    model=model_id, contents=sample_row)\ntuned_tokens_output = tuned_model_output.usage_metadata.candidates_token_count\nprint('Tuned output tokens:', tuned_tokens_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:14:02.614741Z","iopub.execute_input":"2025-04-02T08:14:02.615096Z","iopub.status.idle":"2025-04-02T08:14:03.532486Z","shell.execute_reply.started":"2025-04-02T08:14:02.615061Z","shell.execute_reply":"2025-04-02T08:14:03.531188Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Next steps\n\nNow that you have tuned a classification model, try some other tasks, like tuning a model to respond with a specific tone or style using hand-written examples (or even generated examples!). Kaggle hosts [a number of datasets](https://www.kaggle.com/datasets) you can try out.\n\nLearn about [when supervised fine-tuning is most effective](https://cloud.google.com/blog/products/ai-machine-learning/supervised-fine-tuning-for-gemini-llm).\n\nAnd check out the [fine-tuning tutorial](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?hl=en&lang=python) for another example that shows a tuned model extending beyond the training data to new, unseen inputs.\n\n*- [Mark McD](https://linktr.ee/markmcd)*","metadata":{"id":"6c1204a5d0ab"}}]}